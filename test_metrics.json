{
  "metrics": {
    "coverage_rate": 100.0,
    "success_rate": 55.55555555555556,
    "partial_success_rate": 0.0
  },
  "status": "success",
  "model_name": "accounts/fireworks/models/llama-v3p1-405b-instruct",
  "timestamp": "2025-01-19T00:26:38.938873",
  "results": [
    {
      "test_name": "tests/test_code_generation.py::test_function_implementation",
      "outcome": "passed",
      "error_message": null,
      "duration": 7.692792036,
      "timestamp": "2025-01-19T00:26:10.827027"
    },
    {
      "test_name": "tests/test_code_generation.py::test_algorithm_solutions",
      "outcome": "failed",
      "error_message": "llm_client = <coroutine object llm_client at 0x7ff87ab0ba40>\ndb_session = <sqlalchemy.orm.session.Session object at 0x7ff87abb4d60>\n\n    @pytest.mark.asyncio\n    async def test_algorithm_solutions(llm_client, db_session):\n        \"\"\"Test implementation of common algorithms.\"\"\"\n        client = await llm_client\n    \n        algorithms = [\n            {\n                \"name\": \"Binary Search\",\n                \"prompt\": \"\"\"\n                Implement a binary search function in Python that finds the index of a\n                target value in a sorted list. Return -1 if the target is not found.\n                Return the code in a Python code block.\n                \"\"\",\n                \"requirements\": [\n                    \"Input validation\",\n                    \"Handle empty list\",\n                    \"Return correct index\",\n                    \"Return -1 if not found\"\n                ]\n            },\n            {\n                \"name\": \"Merge Sort\",\n                \"prompt\": \"\"\"\n                Implement the merge sort algorithm in Python to sort a list of numbers\n                in ascending order. Return the code in a Python code block.\n                \"\"\",\n                \"requirements\": [\n                    \"Divide and conquer approach\",\n                    \"Merge function\",\n                    \"Handle empty or single-element lists\",\n                    \"Maintain stable sort\"\n                ]\n            }\n        ]\n    \n        for algo in algorithms:\n            response = await client.generate(\n                prompt=algo[\"prompt\"],\n                max_tokens=500,\n                temperature=0.0\n            )\n    \n            raw_code = response[\"choices\"][0][\"text\"].strip()\n            code = extract_code_block(raw_code)\n    \n            # Verify syntax\n            try:\n                ast.parse(code)\n            except SyntaxError as e:\n                pytest.fail(f\"Generated {algo['name']} code has syntax error: {e}\\nCode:\\n{code}\")\n    \n            # Check for algorithm-specific requirements\n            for req in algo[\"requirements\"]:\n>               assert any(keyword in code.lower() for keyword in req.lower().split()), \\\n                    f\"Missing requirement: {req}\"\nE               AssertionError: Missing requirement: Input validation\nE               assert False\nE                +  where False = any(<generator object test_algorithm_solutions.<locals>.<genexpr> at 0x7ff87ab07660>)\n\ntests/test_code_generation.py:138: AssertionError",
      "duration": 4.235853638000009,
      "timestamp": "2025-01-19T00:26:15.107298"
    },
    {
      "test_name": "tests/test_code_generation.py::test_code_refactoring",
      "outcome": "failed",
      "error_message": "llm_client = <coroutine object llm_client at 0x7ff87a71cac0>\ndb_session = <sqlalchemy.orm.session.Session object at 0x7ff87abb4d60>\n\n    @pytest.mark.asyncio\n    async def test_code_refactoring(llm_client, db_session):\n        \"\"\"Test code refactoring capabilities.\"\"\"\n        client = await llm_client\n    \n        original_code = \"\"\"\n        def process_data(data):\n            result = []\n            for i in range(len(data)):\n                if data[i] > 0:\n                    if data[i] % 2 == 0:\n                        if data[i] < 100:\n                            result.append(data[i] * 2)\n            return result\n        \"\"\"\n    \n        refactor_prompt = f\"\"\"\n        Refactor this code to improve readability and efficiency. The code should:\n        1. Use list comprehension\n        2. Reduce nesting\n        3. Add type hints\n        4. Add docstring\n    \n        Return the refactored code in a Python code block.\n    \n        Original code:\n        {original_code}\n        \"\"\"\n    \n        response = await client.generate(\n            prompt=refactor_prompt,\n            max_tokens=400,\n            temperature=0.0\n        )\n    \n        raw_code = response[\"choices\"][0][\"text\"].strip()\n        refactored_code = extract_code_block(raw_code)\n    \n        # Verify syntax first\n        try:\n            ast.parse(refactored_code)\n        except SyntaxError as e:\n            pytest.fail(f\"Generated code has syntax error: {e}\\nCode:\\n{refactored_code}\")\n    \n        # Check for improvements\n        required_elements = [\n            \"def process_data\",\n            \"List[\",  # Type hints\n            '\"\"\"',    # Docstring\n            \"[\",      # List comprehension\n            \"return\"\n        ]\n    \n        for element in required_elements:\n>           assert element in refactored_code, f\"Missing element in refactored code: {element}\"\nE           AssertionError: Missing element in refactored code: List[\nE           assert 'List[' in 'def process_data(data: list[int]) -> list[int]:\\n    \"\"\"\\n    Process a list of integers by filtering out non-positive numbers, \\n    odd numbers, and numbers greater than or equal to 100, \\n    then doubling the remaining numbers.\\n\\n    Args:\\n        data (list[int]): A list of integers to be processed.\\n\\n    Returns:\\n        list[int]: A list of processed integers.\\n    \"\"\"\\n    return [x * 2 for x in data if x > 0 and x % 2 == 0 and x < 100]'\n\ntests/test_code_generation.py:195: AssertionError",
      "duration": 3.772252180999999,
      "timestamp": "2025-01-19T00:26:18.885712"
    },
    {
      "test_name": "tests/test_code_generation.py::test_error_handling",
      "outcome": "failed",
      "error_message": "llm_client = <coroutine object llm_client at 0x7ff87ab0bac0>\ndb_session = <sqlalchemy.orm.session.Session object at 0x7ff87abb4d60>\n\n    @pytest.mark.asyncio\n    async def test_error_handling(llm_client, db_session):\n        \"\"\"Test generation of code with proper error handling.\"\"\"\n        client = await llm_client\n    \n        prompt = \"\"\"\n        Write a Python function that reads a JSON file and extracts specific fields.\n        The function should handle all possible errors (file not found, invalid JSON,\n        missing fields) gracefully. Return the code in a Python code block.\n        \"\"\"\n    \n        response = await client.generate(\n            prompt=prompt,\n            max_tokens=400,\n            temperature=0.0\n        )\n    \n        raw_code = response[\"choices\"][0][\"text\"].strip()\n        code = extract_code_block(raw_code)\n    \n        # Verify syntax first\n        try:\n            ast.parse(code)\n        except SyntaxError as e:\n            pytest.fail(f\"Generated code has syntax error: {e}\\nCode:\\n{code}\")\n    \n        # Check for error handling constructs\n        required_elements = [\n            \"try:\",\n            \"except\",\n            \"FileNotFoundError\",\n            \"json.JSONDecodeError\",\n            \"raise\"\n        ]\n    \n        for element in required_elements:\n>           assert element in code, f\"Missing error handling element: {element}\"\nE           AssertionError: Missing error handling element: raise\nE           assert 'raise' in 'import json\\n\\ndef extract_fields_from_json(file_path, fields):\\n    \"\"\"\\n    Extracts specific fields from a JSON file.\\n\\n    Args:\\n        file_path (str): The path to the JSON file.\\n        fields (list): A list of fields to extract.\\n\\n    Returns:\\n        dict: A dictionary containing the extracted fields.\\n    \"\"\"\\n    try:\\n        with open(file_path, \\'r\\') as file:\\n            # Attempt to open the file\\n            data = json.load(file)\\n            # Attempt to parse the JSON\\n    except FileNotFoundError:\\n        print(f\"Error: File \\'{file_path}\\' not found.\")\\n        return {}\\n    except json.JSONDecodeError as e:\\n        print(f\"Error: Invalid JSON in file \\'{file_path}\\'. {e}\")\\n        return {}\\n\\n    extracted_fields = {}\\n    for field in fields:\\n        try:\\n            # Attempt to extract the field\\n            extracted_fields[field] = data[field]\\n        except KeyError:\\n            print(f\"Warning: Field \\'{field}\\' not found in file \\'{file_path}\\'.\")\\n\\n    return extracted_fields\\n\\n# Example usage:\\nfile_path = \\'example.json\\'\\nfields = [\\'name\\', \\'age\\', \\' occupation\\']\\n\\nextracted_fields = extract_fields_from_json(file_path, fields)\\nprint(extracted_fields)'\n\ntests/test_code_generation.py:238: AssertionError",
      "duration": 5.258941661000009,
      "timestamp": "2025-01-19T00:26:24.150526"
    },
    {
      "test_name": "tests/test_few_shot.py::test_sentiment_analysis",
      "outcome": "passed",
      "error_message": null,
      "duration": 5.392463075000009,
      "timestamp": "2025-01-19T00:26:29.544634"
    },
    {
      "test_name": "tests/test_few_shot.py::test_topic_classification",
      "outcome": "passed",
      "error_message": null,
      "duration": 1.5891847729999995,
      "timestamp": "2025-01-19T00:26:31.135469"
    },
    {
      "test_name": "tests/test_few_shot.py::test_intent_classification",
      "outcome": "failed",
      "error_message": "llm_client = <coroutine object llm_client at 0x7ff87a887ac0>\ndb_session = <sqlalchemy.orm.session.Session object at 0x7ff87abb4d60>\n\n    @pytest.mark.asyncio\n    async def test_intent_classification(llm_client, db_session):\n        \"\"\"Test user intent classification with few-shot examples.\"\"\"\n        client = await llm_client\n    \n        examples = [\n            {\n                \"text\": \"What's the current status of my order?\",\n                \"intent\": \"check_status\"\n            },\n            {\n                \"text\": \"I need to return this product\",\n                \"intent\": \"return_request\"\n            },\n            {\n                \"text\": \"Do you ship to international locations?\",\n                \"intent\": \"shipping_inquiry\"\n            }\n        ]\n    \n        test_queries = [\n            {\n                \"text\": \"When will my package arrive?\",\n                \"expected\": \"check_status\"\n            },\n            {\n                \"text\": \"How do I send this item back?\",\n                \"expected\": \"return_request\"\n            },\n            {\n                \"text\": \"Can you deliver to Canada?\",\n                \"expected\": \"shipping_inquiry\"\n            }\n        ]\n    \n        # Create few-shot prompt\n        few_shot_prompt = \"Classify the user intent as check_status, return_request, or shipping_inquiry:\\n\\n\"\n        for example in examples:\n            few_shot_prompt += f\"Query: {example['text']}\\nIntent: {example['intent']}\\n\\n\"\n    \n        for query in test_queries:\n            prompt = few_shot_prompt + f\"Query: {query['text']}\\nIntent:\"\n    \n            response = await client.generate(\n                prompt=prompt,\n                max_tokens=50,\n                temperature=0.3\n            )\n    \n            prediction = response[\"choices\"][0][\"text\"].strip().lower()\n>           assert query[\"expected\"] in prediction, \\\n                f\"Expected {query['expected']} but got {prediction}\"\nE           AssertionError: Expected check_status but got shipping_inquiry\nE           assert 'check_status' in 'shipping_inquiry'\n\ntests/test_few_shot.py:156: AssertionError",
      "duration": 0.445049852000011,
      "timestamp": "2025-01-19T00:26:31.587014"
    },
    {
      "test_name": "tests/test_few_shot.py::test_entity_extraction",
      "outcome": "failed",
      "error_message": "llm_client = <coroutine object llm_client at 0x7ff87a89f840>\ndb_session = <sqlalchemy.orm.session.Session object at 0x7ff87abb4d60>\n\n    @pytest.mark.asyncio\n    async def test_entity_extraction(llm_client, db_session):\n        \"\"\"Test named entity extraction with few-shot examples.\"\"\"\n        client = await llm_client\n    \n        examples = [\n            {\n                \"text\": \"John Smith works at Apple Inc. in California\",\n                \"entities\": {\n                    \"person\": \"John Smith\",\n                    \"organization\": \"Apple Inc.\",\n                    \"location\": \"California\"\n                }\n            },\n            {\n                \"text\": \"Microsoft CEO Satya Nadella visited London\",\n                \"entities\": {\n                    \"person\": \"Satya Nadella\",\n                    \"organization\": \"Microsoft\",\n                    \"location\": \"London\"\n                }\n            }\n        ]\n    \n        test_cases = [\n            {\n                \"text\": \"Amazon founder Jeff Bezos spoke in Seattle\",\n                \"expected\": {\n                    \"person\": \"Jeff Bezos\",\n                    \"organization\": \"Amazon\",\n                    \"location\": \"Seattle\"\n                }\n            }\n        ]\n    \n        # Create few-shot prompt\n        few_shot_prompt = \"Extract the person, organization, and location from the text:\\n\\n\"\n        for example in examples:\n            few_shot_prompt += f\"Text: {example['text']}\\n\"\n            few_shot_prompt += f\"Entities: {json.dumps(example['entities'], indent=2)}\\n\\n\"\n    \n        for case in test_cases:\n            prompt = few_shot_prompt + f\"Text: {case['text']}\\nEntities:\"\n    \n            response = await client.generate(\n                prompt=prompt,\n                max_tokens=150,\n                temperature=0.3\n            )\n    \n            # Extract JSON from response\n            try:\n>               prediction = json.loads(response[\"choices\"][0][\"text\"].strip())\n\ntests/test_few_shot.py:211: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/json/__init__.py:346: in loads\n    return _default_decoder.decode(s)\n/opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/json/decoder.py:337: in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <json.decoder.JSONDecoder object at 0x7ff87d322880>\ns = 'Based on the pattern, I can extract the entities as follows:\\n\\nEntities: {\\n  \"person\": \"Jeff Bezos\",\\n  \"organization\": \"Amazon\",\\n  \"location\": \"Seattle\"\\n}'\nidx = 0\n\n    def raw_decode(self, s, idx=0):\n        \"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\n        a JSON document) and return a 2-tuple of the Python\n        representation and the index in ``s`` where the document ended.\n    \n        This can be used to decode a JSON document from a string that may\n        have extraneous data at the end.\n    \n        \"\"\"\n        try:\n            obj, end = self.scan_once(s, idx)\n        except StopIteration as err:\n>           raise JSONDecodeError(\"Expecting value\", s, err.value) from None\nE           json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n/opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/json/decoder.py:355: JSONDecodeError\n\nDuring handling of the above exception, another exception occurred:\n\nllm_client = <coroutine object llm_client at 0x7ff87a89f840>\ndb_session = <sqlalchemy.orm.session.Session object at 0x7ff87abb4d60>\n\n    @pytest.mark.asyncio\n    async def test_entity_extraction(llm_client, db_session):\n        \"\"\"Test named entity extraction with few-shot examples.\"\"\"\n        client = await llm_client\n    \n        examples = [\n            {\n                \"text\": \"John Smith works at Apple Inc. in California\",\n                \"entities\": {\n                    \"person\": \"John Smith\",\n                    \"organization\": \"Apple Inc.\",\n                    \"location\": \"California\"\n                }\n            },\n            {\n                \"text\": \"Microsoft CEO Satya Nadella visited London\",\n                \"entities\": {\n                    \"person\": \"Satya Nadella\",\n                    \"organization\": \"Microsoft\",\n                    \"location\": \"London\"\n                }\n            }\n        ]\n    \n        test_cases = [\n            {\n                \"text\": \"Amazon founder Jeff Bezos spoke in Seattle\",\n                \"expected\": {\n                    \"person\": \"Jeff Bezos\",\n                    \"organization\": \"Amazon\",\n                    \"location\": \"Seattle\"\n                }\n            }\n        ]\n    \n        # Create few-shot prompt\n        few_shot_prompt = \"Extract the person, organization, and location from the text:\\n\\n\"\n        for example in examples:\n            few_shot_prompt += f\"Text: {example['text']}\\n\"\n            few_shot_prompt += f\"Entities: {json.dumps(example['entities'], indent=2)}\\n\\n\"\n    \n        for case in test_cases:\n            prompt = few_shot_prompt + f\"Text: {case['text']}\\nEntities:\"\n    \n            response = await client.generate(\n                prompt=prompt,\n                max_tokens=150,\n                temperature=0.3\n            )\n    \n            # Extract JSON from response\n            try:\n                prediction = json.loads(response[\"choices\"][0][\"text\"].strip())\n                for entity_type, expected_value in case[\"expected\"].items():\n                    assert expected_value.lower() in prediction[entity_type].lower(), \\\n                        f\"Expected {expected_value} in {entity_type}\"\n            except json.JSONDecodeError:\n>               pytest.fail(\"Response is not valid JSON\")\nE               Failed: Response is not valid JSON\n\ntests/test_few_shot.py:216: Failed",
      "duration": 1.478743809000008,
      "timestamp": "2025-01-19T00:26:33.099856"
    },
    {
      "test_name": "tests/test_mathematics.py::test_arithmetic_operations",
      "outcome": "passed",
      "error_message": null,
      "duration": 0.9882641130000138,
      "timestamp": "2025-01-19T00:26:34.089970"
    },
    {
      "test_name": "tests/test_mathematics.py::test_word_problems",
      "outcome": "passed",
      "error_message": null,
      "duration": 0.5290982740000061,
      "timestamp": "2025-01-19T00:26:34.631487"
    },
    {
      "test_name": "tests/test_mathematics.py::test_basic_algebra",
      "outcome": "passed",
      "error_message": null,
      "duration": 0.6159268730000065,
      "timestamp": "2025-01-19T00:26:35.253838"
    },
    {
      "test_name": "tests/test_mathematics.py::test_mathematical_reasoning",
      "outcome": "passed",
      "error_message": null,
      "duration": 0.38416688699999213,
      "timestamp": "2025-01-19T00:26:35.643808"
    },
    {
      "test_name": "tests/test_reasoning.py::test_logical_deduction",
      "outcome": "failed",
      "error_message": "llm_client = <coroutine object llm_client at 0x7ff87ab0bac0>\ndb_session = <sqlalchemy.orm.session.Session object at 0x7ff87abb4d60>\nmake_test_suite = <function make_test_suite.<locals>._make_suite at 0x7ff87a81f700>\n\n    @pytest.mark.asyncio\n    async def test_logical_deduction(llm_client, db_session, make_test_suite):\n        \"\"\"Test logical deduction capabilities.\"\"\"\n        client = await llm_client\n    \n        suite = make_test_suite(\n            name_prefix=\"logical_deduction\",\n            description=\"Tests for logical deduction\",\n            category=\"reasoning\"\n        )\n    \n        premises = \"\"\"\n        1. All programmers like coffee\n        2. Some coffee drinkers work at night\n        3. Alice is a programmer\n        \"\"\"\n    \n        questions = [\n            {\n                \"query\": \"Does Alice like coffee?\",\n                \"expected_keywords\": [\"yes\", \"true\", \"likes\", \"does\"]\n            },\n            {\n                \"query\": \"Do all programmers work at night?\",\n                \"expected_keywords\": [\"cannot\", \"unknown\", \"insufficient\", \"maybe\"]\n            }\n        ]\n    \n        for i, question in enumerate(questions):\n            test = UnitTest(\n                test_id=uuid.uuid4(),\n                suite_id=suite.suite_id,\n                test_name=f\"Deduction Test {i+1}\",\n                test_type=\"logical_deduction\",\n                input_data={\n                    \"premises\": premises,\n                    \"question\": question[\"query\"]\n                },\n                expected_output={\"keywords\": question[\"expected_keywords\"]}\n            )\n            db_session.add(test)\n        db_session.commit()\n    \n        for question in questions:\n            prompt = f\"\"\"\n            Given these premises:\n            {premises}\n    \n            Question: {question[\"query\"]}\n            Provide a direct answer without explanation.\n            \"\"\"\n    \n            response = await client.generate(\n                prompt=prompt,\n                max_tokens=50,\n                temperature=0.0\n            )\n    \n            answer = response[\"choices\"][0][\"text\"].strip().lower()\n>           assert any(keyword in answer for keyword in question[\"expected_keywords\"]), \\\n                f\"Answer '{answer}' does not contain any expected keywords: {question['expected_keywords']}\"\nE           AssertionError: Answer 'no.' does not contain any expected keywords: ['cannot', 'unknown', 'insufficient', 'maybe']\nE           assert False\nE            +  where False = any(<generator object test_logical_deduction.<locals>.<genexpr> at 0x7ff87a7ff9e0>)\n\ntests/test_reasoning.py:68: AssertionError",
      "duration": 0.7305358339999941,
      "timestamp": "2025-01-19T00:26:36.385991"
    },
    {
      "test_name": "tests/test_reasoning.py::test_cause_effect_analysis",
      "outcome": "passed",
      "error_message": null,
      "duration": 0.9138838709999959,
      "timestamp": "2025-01-19T00:26:37.306827"
    },
    {
      "test_name": "tests/test_reasoning.py::test_analogical_reasoning",
      "outcome": "failed",
      "error_message": "llm_client = <coroutine object llm_client at 0x7ff87a814dc0>\ndb_session = <sqlalchemy.orm.session.Session object at 0x7ff87abb4d60>\nmake_test_suite = <function make_test_suite.<locals>._make_suite at 0x7ff87a81fb80>\n\n    @pytest.mark.asyncio\n    async def test_analogical_reasoning(llm_client, db_session, make_test_suite):\n        \"\"\"Test analogical reasoning capabilities.\"\"\"\n        client = await llm_client\n    \n        suite = make_test_suite(\n            name_prefix=\"analogical\",\n            description=\"Tests for analogical reasoning\",\n            category=\"reasoning\"\n        )\n    \n        analogies = [\n            {\n                \"prompt\": \"Teacher is to Student as Doctor is to:\",\n                \"expected\": \"patient\",\n                \"incorrect\": [\"nurse\", \"medicine\", \"hospital\"]\n            }\n        ]\n    \n        for i, analogy in enumerate(analogies):\n            test = UnitTest(\n                test_id=uuid.uuid4(),\n                suite_id=suite.suite_id,\n                test_name=f\"Analogy Test {i+1}\",\n                test_type=\"analogy\",\n                input_data={\"prompt\": analogy[\"prompt\"]},\n                expected_output={\n                    \"answer\": analogy[\"expected\"],\n                    \"incorrect\": analogy[\"incorrect\"]\n                }\n            )\n            db_session.add(test)\n        db_session.commit()\n    \n        for analogy in analogies:\n            response = await client.generate(\n                prompt=f\"{analogy['prompt']} Provide just the one-word answer.\",\n                max_tokens=50,\n                temperature=0.0\n            )\n    \n            answer = response[\"choices\"][0][\"text\"].strip().lower()\n>           assert answer == analogy[\"expected\"], \\\n                f\"Expected {analogy['expected']} but got {answer}\"\nE           AssertionError: Expected patient but got patient.\nE           assert 'patient.' == 'patient'\nE             - patient\nE             + patient.\nE             ?        +\n\ntests/test_reasoning.py:171: AssertionError",
      "duration": 0.515182067000012,
      "timestamp": "2025-01-19T00:26:37.831104"
    },
    {
      "test_name": "tests/test_summarization.py::test_article_summarization",
      "outcome": "failed",
      "error_message": "llm_client = <coroutine object llm_client at 0x7ff87a887540>\ndb_session = <sqlalchemy.orm.session.Session object at 0x7ff87abb4d60>\nmake_test_suite = <function make_test_suite.<locals>._make_suite at 0x7ff87a8d9670>\n\n    @pytest.mark.asyncio\n    async def test_article_summarization(llm_client, db_session, make_test_suite):\n        \"\"\"Test article summarization capabilities.\"\"\"\n        # Need to await our async fixture\n        client = await llm_client\n    \n        # Create a uniquely named test suite\n        suite = make_test_suite(\n            name_prefix=\"summarization\",\n            description=\"Tests for text summarization capabilities\",\n            category=\"summarization\"\n        )\n    \n        # Test article\n        article = \"\"\"\n        The James Webb Space Telescope has revolutionized our view of the cosmos.\n        Launched in December 2021, this $10 billion observatory has provided\n        unprecedented views of distant galaxies, star-forming regions, and\n        exoplanets. Its infrared capabilities allow it to peer through cosmic\n        dust and see light from the earliest galaxies in the universe.\n        \"\"\"\n    \n        # Define expected key points\n        key_points = [\n            \"James Webb Space Telescope\",\n            \"launched December 2021\",\n            \"infrared observation\",\n            \"galaxies and exoplanets\"\n        ]\n    \n        # Create test case in database\n        test_case = UnitTest(\n            test_id=uuid.uuid4(),\n            suite_id=suite.suite_id,\n            test_name=\"Article Summarization\",\n            test_type=\"summarization\",\n            test_description=\"Test summarization of a scientific article\",\n            input_data={\"article\": article},\n            expected_output={\"key_points\": key_points}\n        )\n        db_session.add(test_case)\n        db_session.commit()\n    \n        # Get summary from model\n        response = await client.generate(\n            prompt=f\"Please summarize this article concisely: {article}\",\n            max_tokens=100,\n            temperature=0.3\n        )\n    \n        summary = response[\"choices\"][0][\"text\"].strip()\n    \n        # Check if key points are present\n        for point in key_points:\n>           assert point.lower() in summary.lower(), f\"Missing key point: {point}\"\nE           AssertionError: Missing key point: launched December 2021\nE           assert 'launched december 2021' in 'here is a concise summary of the article:\\n\\nthe james webb space telescope, launched in 2021, has greatly expanded our understanding of the universe with its unparalleled views of galaxies, star-forming regions, and exoplanets, thanks to its infrared capabilities that can penetrate cosmic dust.'\nE            +  where 'launched december 2021' = <built-in method lower of str object at 0x7ff87abb2440>()\nE            +    where <built-in method lower of str object at 0x7ff87abb2440> = 'launched December 2021'.lower\nE            +  and   'here is a concise summary of the article:\\n\\nthe james webb space telescope, launched in 2021, has greatly expanded our understanding of the universe with its unparalleled views of galaxies, star-forming regions, and exoplanets, thanks to its infrared capabilities that can penetrate cosmic dust.' = <built-in method lower of str object at 0x7ff87a8c5510>()\nE            +    where <built-in method lower of str object at 0x7ff87a8c5510> = 'Here is a concise summary of the article:\\n\\nThe James Webb Space Telescope, launched in 2021, has greatly expanded our understanding of the universe with its unparalleled views of galaxies, star-forming regions, and exoplanets, thanks to its infrared capabilities that can penetrate cosmic dust.'.lower\n\ntests/test_summarization.py:64: AssertionError",
      "duration": 1.089956285999989,
      "timestamp": "2025-01-19T00:26:38.930480"
    },
    {
      "test_name": "tests/test_summarization.py::test_multi_document_summarization",
      "outcome": "passed",
      "error_message": null,
      "duration": 0.00022083100000713785,
      "timestamp": "2025-01-19T00:26:38.935970"
    },
    {
      "test_name": "tests/test_summarization.py::test_bullet_point_extraction",
      "outcome": "passed",
      "error_message": null,
      "duration": 0.00019172699998648568,
      "timestamp": "2025-01-19T00:26:38.937321"
    }
  ]
}