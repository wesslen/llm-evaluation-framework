{
  "metrics": {
    "coverage_rate": 100.0,
    "success_rate": 55.55555555555556,
    "partial_success_rate": 0.0
  },
  "status": "success",
  "model_name": "accounts/fireworks/models/llama-v3p3-70b-instruct",
  "timestamp": "2025-01-18T23:50:12.448268",
  "results": [
    {
      "test_name": "tests/test_code_generation.py::test_function_implementation",
      "outcome": "passed",
      "error_message": null,
      "duration": 3.140400710999998,
      "timestamp": "2025-01-18T23:49:55.516761"
    },
    {
      "test_name": "tests/test_code_generation.py::test_algorithm_solutions",
      "outcome": "failed",
      "error_message": "llm_client = <coroutine object llm_client at 0x7f4c30109a40>\ndb_session = <sqlalchemy.orm.session.Session object at 0x7f4c300e9ca0>\n\n    @pytest.mark.asyncio\n    async def test_algorithm_solutions(llm_client, db_session):\n        \"\"\"Test implementation of common algorithms.\"\"\"\n        client = await llm_client\n    \n        algorithms = [\n            {\n                \"name\": \"Binary Search\",\n                \"prompt\": \"\"\"\n                Implement a binary search function in Python that finds the index of a\n                target value in a sorted list. Return -1 if the target is not found.\n                Return the code in a Python code block.\n                \"\"\",\n                \"requirements\": [\n                    \"Input validation\",\n                    \"Handle empty list\",\n                    \"Return correct index\",\n                    \"Return -1 if not found\"\n                ]\n            },\n            {\n                \"name\": \"Merge Sort\",\n                \"prompt\": \"\"\"\n                Implement the merge sort algorithm in Python to sort a list of numbers\n                in ascending order. Return the code in a Python code block.\n                \"\"\",\n                \"requirements\": [\n                    \"Divide and conquer approach\",\n                    \"Merge function\",\n                    \"Handle empty or single-element lists\",\n                    \"Maintain stable sort\"\n                ]\n            }\n        ]\n    \n        for algo in algorithms:\n            response = await client.generate(\n                prompt=algo[\"prompt\"],\n                max_tokens=500,\n                temperature=0.0\n            )\n    \n            raw_code = response[\"choices\"][0][\"text\"].strip()\n            code = extract_code_block(raw_code)\n    \n            # Verify syntax\n            try:\n                ast.parse(code)\n            except SyntaxError as e:\n                pytest.fail(f\"Generated {algo['name']} code has syntax error: {e}\\nCode:\\n{code}\")\n    \n            # Check for algorithm-specific requirements\n            for req in algo[\"requirements\"]:\n>               assert any(keyword in code.lower() for keyword in req.lower().split()), \\\n                    f\"Missing requirement: {req}\"\nE               AssertionError: Missing requirement: Input validation\nE               assert False\nE                +  where False = any(<generator object test_algorithm_solutions.<locals>.<genexpr> at 0x7f4c30106660>)\n\ntests/test_code_generation.py:138: AssertionError",
      "duration": 2.460140408000001,
      "timestamp": "2025-01-18T23:49:58.021418"
    },
    {
      "test_name": "tests/test_code_generation.py::test_code_refactoring",
      "outcome": "failed",
      "error_message": "llm_client = <coroutine object llm_client at 0x7f4c2fd1cac0>\ndb_session = <sqlalchemy.orm.session.Session object at 0x7f4c300e9ca0>\n\n    @pytest.mark.asyncio\n    async def test_code_refactoring(llm_client, db_session):\n        \"\"\"Test code refactoring capabilities.\"\"\"\n        client = await llm_client\n    \n        original_code = \"\"\"\n        def process_data(data):\n            result = []\n            for i in range(len(data)):\n                if data[i] > 0:\n                    if data[i] % 2 == 0:\n                        if data[i] < 100:\n                            result.append(data[i] * 2)\n            return result\n        \"\"\"\n    \n        refactor_prompt = f\"\"\"\n        Refactor this code to improve readability and efficiency. The code should:\n        1. Use list comprehension\n        2. Reduce nesting\n        3. Add type hints\n        4. Add docstring\n    \n        Return the refactored code in a Python code block.\n    \n        Original code:\n        {original_code}\n        \"\"\"\n    \n        response = await client.generate(\n            prompt=refactor_prompt,\n            max_tokens=400,\n            temperature=0.0\n        )\n    \n        raw_code = response[\"choices\"][0][\"text\"].strip()\n        refactored_code = extract_code_block(raw_code)\n    \n        # Verify syntax first\n        try:\n            ast.parse(refactored_code)\n        except SyntaxError as e:\n            pytest.fail(f\"Generated code has syntax error: {e}\\nCode:\\n{refactored_code}\")\n    \n        # Check for improvements\n        required_elements = [\n            \"def process_data\",\n            \"List[\",  # Type hints\n            '\"\"\"',    # Docstring\n            \"[\",      # List comprehension\n            \"return\"\n        ]\n    \n        for element in required_elements:\n>           assert element in refactored_code, f\"Missing element in refactored code: {element}\"\nE           AssertionError: Missing element in refactored code: List[\nE           assert 'List[' in 'def process_data(data: list[int]) -> list[int]:\\n    \"\"\"\\n    This function processes a list of integers, filtering out non-positive numbers, \\n    odd numbers, and numbers greater than or equal to 100. It then doubles the \\n    remaining numbers and returns them in a new list.\\n\\n    Args:\\n        data (list[int]): A list of integers to be processed.\\n\\n    Returns:\\n        list[int]: A list of doubled integers after filtering.\\n    \"\"\"\\n    # Use list comprehension to filter and double the numbers in a single step\\n    return [num * 2 for num in data if num > 0 and num % 2 == 0 and num < 100]'\n\ntests/test_code_generation.py:195: AssertionError",
      "duration": 1.3305786740000087,
      "timestamp": "2025-01-18T23:49:59.358262"
    },
    {
      "test_name": "tests/test_code_generation.py::test_error_handling",
      "outcome": "failed",
      "error_message": "llm_client = <coroutine object llm_client at 0x7f4c30109ac0>\ndb_session = <sqlalchemy.orm.session.Session object at 0x7f4c300e9ca0>\n\n    @pytest.mark.asyncio\n    async def test_error_handling(llm_client, db_session):\n        \"\"\"Test generation of code with proper error handling.\"\"\"\n        client = await llm_client\n    \n        prompt = \"\"\"\n        Write a Python function that reads a JSON file and extracts specific fields.\n        The function should handle all possible errors (file not found, invalid JSON,\n        missing fields) gracefully. Return the code in a Python code block.\n        \"\"\"\n    \n        response = await client.generate(\n            prompt=prompt,\n            max_tokens=400,\n            temperature=0.0\n        )\n    \n        raw_code = response[\"choices\"][0][\"text\"].strip()\n        code = extract_code_block(raw_code)\n    \n        # Verify syntax first\n        try:\n            ast.parse(code)\n        except SyntaxError as e:\n            pytest.fail(f\"Generated code has syntax error: {e}\\nCode:\\n{code}\")\n    \n        # Check for error handling constructs\n        required_elements = [\n            \"try:\",\n            \"except\",\n            \"FileNotFoundError\",\n            \"json.JSONDecodeError\",\n            \"raise\"\n        ]\n    \n        for element in required_elements:\n>           assert element in code, f\"Missing error handling element: {element}\"\nE           AssertionError: Missing error handling element: raise\nE           assert 'raise' in 'import json\\n\\ndef extract_fields_from_json(file_path, fields):\\n    \"\"\"\\n    Reads a JSON file and extracts specific fields.\\n\\n    Args:\\n        file_path (str): The path to the JSON file.\\n        fields (list): A list of fields to extract.\\n\\n    Returns:\\n        dict: A dictionary containing the extracted fields. If a field is missing, it will be None.\\n    \"\"\"\\n\\n    try:\\n        # Attempt to open the file\\n        with open(file_path, \\'r\\') as file:\\n            # Attempt to parse the JSON\\n            data = json.load(file)\\n    except FileNotFoundError:\\n        print(f\"Error: The file \\'{file_path}\\' was not found.\")\\n        return {}\\n    except json.JSONDecodeError as e:\\n        print(f\"Error: Invalid JSON in file \\'{file_path}\\': {e}\")\\n        return {}\\n\\n    # Initialize a dictionary to store the extracted fields\\n    extracted_fields = {}\\n\\n    # Iterate over the fields to extract\\n    for field in fields:\\n        # Attempt to extract the field\\n        try:\\n            extracted_fields[field] = data[field]\\n        except KeyError:\\n            # If the field is missing, set it to None\\n            extracted_fields[field] = None\\n\\n    return extracted_fields\\n\\n\\n# Example usage:\\nif __name__ == \"__main__\":\\n    file_path = \"example.json\"\\n    fields = [\"name\", \"age\", \" occupation\"]\\n\\n    extracted_fields = extract_fields_from_json(file_path, fields)\\n    print(extracted_fields)'\n\ntests/test_code_generation.py:238: AssertionError",
      "duration": 2.6422682240000057,
      "timestamp": "2025-01-18T23:50:02.006556"
    },
    {
      "test_name": "tests/test_few_shot.py::test_sentiment_analysis",
      "outcome": "passed",
      "error_message": null,
      "duration": 2.091276985999997,
      "timestamp": "2025-01-18T23:50:04.099635"
    },
    {
      "test_name": "tests/test_few_shot.py::test_topic_classification",
      "outcome": "passed",
      "error_message": null,
      "duration": 2.1196104039999994,
      "timestamp": "2025-01-18T23:50:06.220938"
    },
    {
      "test_name": "tests/test_few_shot.py::test_intent_classification",
      "outcome": "failed",
      "error_message": "llm_client = <coroutine object llm_client at 0x7f4c2fe86ac0>\ndb_session = <sqlalchemy.orm.session.Session object at 0x7f4c300e9ca0>\n\n    @pytest.mark.asyncio\n    async def test_intent_classification(llm_client, db_session):\n        \"\"\"Test user intent classification with few-shot examples.\"\"\"\n        client = await llm_client\n    \n        examples = [\n            {\n                \"text\": \"What's the current status of my order?\",\n                \"intent\": \"check_status\"\n            },\n            {\n                \"text\": \"I need to return this product\",\n                \"intent\": \"return_request\"\n            },\n            {\n                \"text\": \"Do you ship to international locations?\",\n                \"intent\": \"shipping_inquiry\"\n            }\n        ]\n    \n        test_queries = [\n            {\n                \"text\": \"When will my package arrive?\",\n                \"expected\": \"check_status\"\n            },\n            {\n                \"text\": \"How do I send this item back?\",\n                \"expected\": \"return_request\"\n            },\n            {\n                \"text\": \"Can you deliver to Canada?\",\n                \"expected\": \"shipping_inquiry\"\n            }\n        ]\n    \n        # Create few-shot prompt\n        few_shot_prompt = \"Classify the user intent as check_status, return_request, or shipping_inquiry:\\n\\n\"\n        for example in examples:\n            few_shot_prompt += f\"Query: {example['text']}\\nIntent: {example['intent']}\\n\\n\"\n    \n        for query in test_queries:\n            prompt = few_shot_prompt + f\"Query: {query['text']}\\nIntent:\"\n    \n            response = await client.generate(\n                prompt=prompt,\n                max_tokens=50,\n                temperature=0.3\n            )\n    \n            prediction = response[\"choices\"][0][\"text\"].strip().lower()\n>           assert query[\"expected\"] in prediction, \\\n                f\"Expected {query['expected']} but got {prediction}\"\nE           AssertionError: Expected check_status but got based on the query, the intent is: shipping_inquiry\nE             \nE             the user is inquiring about the arrival of their package, which is related to the shipping process. this is similar to the example query \"do you ship to international locations?\", which was\nE           assert 'check_status' in 'based on the query, the intent is: shipping_inquiry\\n\\nthe user is inquiring about the arrival of their package, which is related to the shipping process. this is similar to the example query \"do you ship to international locations?\", which was'\n\ntests/test_few_shot.py:156: AssertionError",
      "duration": 0.8814210459999998,
      "timestamp": "2025-01-18T23:50:07.108895"
    },
    {
      "test_name": "tests/test_few_shot.py::test_entity_extraction",
      "outcome": "failed",
      "error_message": "llm_client = <coroutine object llm_client at 0x7f4c2fea9840>\ndb_session = <sqlalchemy.orm.session.Session object at 0x7f4c300e9ca0>\n\n    @pytest.mark.asyncio\n    async def test_entity_extraction(llm_client, db_session):\n        \"\"\"Test named entity extraction with few-shot examples.\"\"\"\n        client = await llm_client\n    \n        examples = [\n            {\n                \"text\": \"John Smith works at Apple Inc. in California\",\n                \"entities\": {\n                    \"person\": \"John Smith\",\n                    \"organization\": \"Apple Inc.\",\n                    \"location\": \"California\"\n                }\n            },\n            {\n                \"text\": \"Microsoft CEO Satya Nadella visited London\",\n                \"entities\": {\n                    \"person\": \"Satya Nadella\",\n                    \"organization\": \"Microsoft\",\n                    \"location\": \"London\"\n                }\n            }\n        ]\n    \n        test_cases = [\n            {\n                \"text\": \"Amazon founder Jeff Bezos spoke in Seattle\",\n                \"expected\": {\n                    \"person\": \"Jeff Bezos\",\n                    \"organization\": \"Amazon\",\n                    \"location\": \"Seattle\"\n                }\n            }\n        ]\n    \n        # Create few-shot prompt\n        few_shot_prompt = \"Extract the person, organization, and location from the text:\\n\\n\"\n        for example in examples:\n            few_shot_prompt += f\"Text: {example['text']}\\n\"\n            few_shot_prompt += f\"Entities: {json.dumps(example['entities'], indent=2)}\\n\\n\"\n    \n        for case in test_cases:\n            prompt = few_shot_prompt + f\"Text: {case['text']}\\nEntities:\"\n    \n            response = await client.generate(\n                prompt=prompt,\n                max_tokens=150,\n                temperature=0.3\n            )\n    \n            # Extract JSON from response\n            try:\n>               prediction = json.loads(response[\"choices\"][0][\"text\"].strip())\n\ntests/test_few_shot.py:211: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/json/__init__.py:346: in loads\n    return _default_decoder.decode(s)\n/opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/json/decoder.py:337: in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <json.decoder.JSONDecoder object at 0x7f4c32918880>\ns = 'Based on the pattern, the extracted entities for the third text would be:\\n\\nEntities: {\\n  \"person\": \"Jeff Bezos\",\\n  \"organization\": \"Amazon\",\\n  \"location\": \"Seattle\"\\n}'\nidx = 0\n\n    def raw_decode(self, s, idx=0):\n        \"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\n        a JSON document) and return a 2-tuple of the Python\n        representation and the index in ``s`` where the document ended.\n    \n        This can be used to decode a JSON document from a string that may\n        have extraneous data at the end.\n    \n        \"\"\"\n        try:\n            obj, end = self.scan_once(s, idx)\n        except StopIteration as err:\n>           raise JSONDecodeError(\"Expecting value\", s, err.value) from None\nE           json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n/opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/json/decoder.py:355: JSONDecodeError\n\nDuring handling of the above exception, another exception occurred:\n\nllm_client = <coroutine object llm_client at 0x7f4c2fea9840>\ndb_session = <sqlalchemy.orm.session.Session object at 0x7f4c300e9ca0>\n\n    @pytest.mark.asyncio\n    async def test_entity_extraction(llm_client, db_session):\n        \"\"\"Test named entity extraction with few-shot examples.\"\"\"\n        client = await llm_client\n    \n        examples = [\n            {\n                \"text\": \"John Smith works at Apple Inc. in California\",\n                \"entities\": {\n                    \"person\": \"John Smith\",\n                    \"organization\": \"Apple Inc.\",\n                    \"location\": \"California\"\n                }\n            },\n            {\n                \"text\": \"Microsoft CEO Satya Nadella visited London\",\n                \"entities\": {\n                    \"person\": \"Satya Nadella\",\n                    \"organization\": \"Microsoft\",\n                    \"location\": \"London\"\n                }\n            }\n        ]\n    \n        test_cases = [\n            {\n                \"text\": \"Amazon founder Jeff Bezos spoke in Seattle\",\n                \"expected\": {\n                    \"person\": \"Jeff Bezos\",\n                    \"organization\": \"Amazon\",\n                    \"location\": \"Seattle\"\n                }\n            }\n        ]\n    \n        # Create few-shot prompt\n        few_shot_prompt = \"Extract the person, organization, and location from the text:\\n\\n\"\n        for example in examples:\n            few_shot_prompt += f\"Text: {example['text']}\\n\"\n            few_shot_prompt += f\"Entities: {json.dumps(example['entities'], indent=2)}\\n\\n\"\n    \n        for case in test_cases:\n            prompt = few_shot_prompt + f\"Text: {case['text']}\\nEntities:\"\n    \n            response = await client.generate(\n                prompt=prompt,\n                max_tokens=150,\n                temperature=0.3\n            )\n    \n            # Extract JSON from response\n            try:\n                prediction = json.loads(response[\"choices\"][0][\"text\"].strip())\n                for entity_type, expected_value in case[\"expected\"].items():\n                    assert expected_value.lower() in prediction[entity_type].lower(), \\\n                        f\"Expected {expected_value} in {entity_type}\"\n            except json.JSONDecodeError:\n>               pytest.fail(\"Response is not valid JSON\")\nE               Failed: Response is not valid JSON\n\ntests/test_few_shot.py:216: Failed",
      "duration": 1.3404318250000102,
      "timestamp": "2025-01-18T23:50:08.505527"
    },
    {
      "test_name": "tests/test_mathematics.py::test_arithmetic_operations",
      "outcome": "failed",
      "error_message": "llm_client = <coroutine object llm_client at 0x7f4c30109ac0>\ndb_session = <sqlalchemy.orm.session.Session object at 0x7f4c300e9ca0>\nmake_test_suite = <function make_test_suite.<locals>._make_suite at 0x7f4c2fe7d820>\n\n    @pytest.mark.asyncio\n    async def test_arithmetic_operations(llm_client, db_session, make_test_suite):\n        \"\"\"Test basic arithmetic operations.\"\"\"\n        client = await llm_client\n    \n        suite = make_test_suite(\n            name_prefix=\"arithmetic\",\n            description=\"Tests for basic arithmetic operations\",\n            category=\"mathematics\"\n        )\n    \n        test_cases = [\n            {\n                \"prompt\": \"What is the result of multiplying 23.5 by 8.75?\",\n                \"expected\": 205.625\n            },\n            {\n                \"prompt\": \"What is 1234 divided by 56.5?\",\n                \"expected\": 21.84070796460177\n            }\n        ]\n    \n        for i, case in enumerate(test_cases):\n            test = UnitTest(\n                test_id=uuid.uuid4(),\n                suite_id=suite.suite_id,\n                test_name=f\"Arithmetic Test {i+1}\",\n                test_type=\"arithmetic\",\n                input_data={\"prompt\": case[\"prompt\"]},\n                expected_output={\"result\": case[\"expected\"]}\n            )\n            db_session.add(test)\n        db_session.commit()\n    \n        for case in test_cases:\n            response = await client.generate(\n                prompt=f\"{case['prompt']} Provide only the numerical answer without any explanation.\",\n                max_tokens=50,\n                temperature=0.0\n            )\n    \n            result = float(response[\"choices\"][0][\"text\"].strip().split()[0])\n>           assert abs(result - case[\"expected\"]) < 0.01, \\\n                f\"Expected {case['expected']} but got {result}\"\nE           AssertionError: Expected 205.625 but got 204.875\nE           assert 0.75 < 0.01\nE            +  where 0.75 = abs((204.875 - 205.625))\n\ntests/test_mathematics.py:52: AssertionError",
      "duration": 0.4403612909999879,
      "timestamp": "2025-01-18T23:50:08.952723"
    },
    {
      "test_name": "tests/test_mathematics.py::test_word_problems",
      "outcome": "passed",
      "error_message": null,
      "duration": 0.3971140139999818,
      "timestamp": "2025-01-18T23:50:09.360740"
    },
    {
      "test_name": "tests/test_mathematics.py::test_basic_algebra",
      "outcome": "passed",
      "error_message": null,
      "duration": 0.37985692400002335,
      "timestamp": "2025-01-18T23:50:09.746718"
    },
    {
      "test_name": "tests/test_mathematics.py::test_mathematical_reasoning",
      "outcome": "passed",
      "error_message": null,
      "duration": 0.3959241549999888,
      "timestamp": "2025-01-18T23:50:10.148952"
    },
    {
      "test_name": "tests/test_reasoning.py::test_logical_deduction",
      "outcome": "failed",
      "error_message": "llm_client = <coroutine object llm_client at 0x7f4c2fd6b5c0>\ndb_session = <sqlalchemy.orm.session.Session object at 0x7f4c300e9ca0>\nmake_test_suite = <function make_test_suite.<locals>._make_suite at 0x7f4c2fe28dc0>\n\n    @pytest.mark.asyncio\n    async def test_logical_deduction(llm_client, db_session, make_test_suite):\n        \"\"\"Test logical deduction capabilities.\"\"\"\n        client = await llm_client\n    \n        suite = make_test_suite(\n            name_prefix=\"logical_deduction\",\n            description=\"Tests for logical deduction\",\n            category=\"reasoning\"\n        )\n    \n        premises = \"\"\"\n        1. All programmers like coffee\n        2. Some coffee drinkers work at night\n        3. Alice is a programmer\n        \"\"\"\n    \n        questions = [\n            {\n                \"query\": \"Does Alice like coffee?\",\n                \"expected_keywords\": [\"yes\", \"true\", \"likes\", \"does\"]\n            },\n            {\n                \"query\": \"Do all programmers work at night?\",\n                \"expected_keywords\": [\"cannot\", \"unknown\", \"insufficient\", \"maybe\"]\n            }\n        ]\n    \n        for i, question in enumerate(questions):\n            test = UnitTest(\n                test_id=uuid.uuid4(),\n                suite_id=suite.suite_id,\n                test_name=f\"Deduction Test {i+1}\",\n                test_type=\"logical_deduction\",\n                input_data={\n                    \"premises\": premises,\n                    \"question\": question[\"query\"]\n                },\n                expected_output={\"keywords\": question[\"expected_keywords\"]}\n            )\n            db_session.add(test)\n        db_session.commit()\n    \n        for question in questions:\n            prompt = f\"\"\"\n            Given these premises:\n            {premises}\n    \n            Question: {question[\"query\"]}\n            Provide a direct answer without explanation.\n            \"\"\"\n    \n            response = await client.generate(\n                prompt=prompt,\n                max_tokens=50,\n                temperature=0.0\n            )\n    \n            answer = response[\"choices\"][0][\"text\"].strip().lower()\n>           assert any(keyword in answer for keyword in question[\"expected_keywords\"]), \\\n                f\"Answer '{answer}' does not contain any expected keywords: {question['expected_keywords']}\"\nE           AssertionError: Answer 'no.' does not contain any expected keywords: ['cannot', 'unknown', 'insufficient', 'maybe']\nE           assert False\nE            +  where False = any(<generator object test_logical_deduction.<locals>.<genexpr> at 0x7f4c2fe06dd0>)\n\ntests/test_reasoning.py:68: AssertionError",
      "duration": 0.7229130220000002,
      "timestamp": "2025-01-18T23:50:10.884515"
    },
    {
      "test_name": "tests/test_reasoning.py::test_cause_effect_analysis",
      "outcome": "passed",
      "error_message": null,
      "duration": 0.518747655999988,
      "timestamp": "2025-01-18T23:50:11.409515"
    },
    {
      "test_name": "tests/test_reasoning.py::test_analogical_reasoning",
      "outcome": "passed",
      "error_message": null,
      "duration": 0.2934494749999885,
      "timestamp": "2025-01-18T23:50:11.708849"
    },
    {
      "test_name": "tests/test_summarization.py::test_article_summarization",
      "outcome": "failed",
      "error_message": "llm_client = <coroutine object llm_client at 0x7f4c2fe86540>\ndb_session = <sqlalchemy.orm.session.Session object at 0x7f4c300e9ca0>\nmake_test_suite = <function make_test_suite.<locals>._make_suite at 0x7f4c2fed0430>\n\n    @pytest.mark.asyncio\n    async def test_article_summarization(llm_client, db_session, make_test_suite):\n        \"\"\"Test article summarization capabilities.\"\"\"\n        # Need to await our async fixture\n        client = await llm_client\n    \n        # Create a uniquely named test suite\n        suite = make_test_suite(\n            name_prefix=\"summarization\",\n            description=\"Tests for text summarization capabilities\",\n            category=\"summarization\"\n        )\n    \n        # Test article\n        article = \"\"\"\n        The James Webb Space Telescope has revolutionized our view of the cosmos.\n        Launched in December 2021, this $10 billion observatory has provided\n        unprecedented views of distant galaxies, star-forming regions, and\n        exoplanets. Its infrared capabilities allow it to peer through cosmic\n        dust and see light from the earliest galaxies in the universe.\n        \"\"\"\n    \n        # Define expected key points\n        key_points = [\n            \"James Webb Space Telescope\",\n            \"launched December 2021\",\n            \"infrared observation\",\n            \"galaxies and exoplanets\"\n        ]\n    \n        # Create test case in database\n        test_case = UnitTest(\n            test_id=uuid.uuid4(),\n            suite_id=suite.suite_id,\n            test_name=\"Article Summarization\",\n            test_type=\"summarization\",\n            test_description=\"Test summarization of a scientific article\",\n            input_data={\"article\": article},\n            expected_output={\"key_points\": key_points}\n        )\n        db_session.add(test_case)\n        db_session.commit()\n    \n        # Get summary from model\n        response = await client.generate(\n            prompt=f\"Please summarize this article concisely: {article}\",\n            max_tokens=100,\n            temperature=0.3\n        )\n    \n        summary = response[\"choices\"][0][\"text\"].strip()\n    \n        # Check if key points are present\n        for point in key_points:\n>           assert point.lower() in summary.lower(), f\"Missing key point: {point}\"\nE           AssertionError: Missing key point: launched December 2021\nE           assert 'launched december 2021' in 'the james webb space telescope, launched in 2021, has transformed our understanding of the universe with its infrared capabilities, providing unprecedented views of distant galaxies, star-forming regions, and exoplanets.'\nE            +  where 'launched december 2021' = <built-in method lower of str object at 0x7f4c301b3300>()\nE            +    where <built-in method lower of str object at 0x7f4c301b3300> = 'launched December 2021'.lower\nE            +  and   'the james webb space telescope, launched in 2021, has transformed our understanding of the universe with its infrared capabilities, providing unprecedented views of distant galaxies, star-forming regions, and exoplanets.' = <built-in method lower of str object at 0x7f4c30196cf0>()\nE            +    where <built-in method lower of str object at 0x7f4c30196cf0> = 'The James Webb Space Telescope, launched in 2021, has transformed our understanding of the universe with its infrared capabilities, providing unprecedented views of distant galaxies, star-forming regions, and exoplanets.'.lower\n\ntests/test_summarization.py:64: AssertionError",
      "duration": 0.7187082000000089,
      "timestamp": "2025-01-18T23:50:12.438838"
    },
    {
      "test_name": "tests/test_summarization.py::test_multi_document_summarization",
      "outcome": "passed",
      "error_message": null,
      "duration": 0.0002572499999757838,
      "timestamp": "2025-01-18T23:50:12.445314"
    },
    {
      "test_name": "tests/test_summarization.py::test_bullet_point_extraction",
      "outcome": "passed",
      "error_message": null,
      "duration": 0.00019666699998310833,
      "timestamp": "2025-01-18T23:50:12.446675"
    }
  ]
}